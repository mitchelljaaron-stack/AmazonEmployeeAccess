folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(trees(range = c(50, 300)), levels = 4)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./BARTPreds.csv", delim = ",")
library(agua)
library(bonsai)
library(lightgbm)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Initialize an h2o session
h2o::h2o.init()
qt(p = .95, df = 8, lower.tail = TRUE)
qt(p = .05, df = 8, lower.tail = TRUE)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
train_data <- train_data %>%
mutate(
ACTION = as.factor(ACTION),
across(where(is.numeric) & !all_of("ACTION"), as.factor)
)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
amazon_predictions <- predict(logReg_wf,
new_data=testData,
type="class")
amazon_predictions <- predict(logReg_wf,
new_data=testData,
type="class")
amazon_predictions <- predict(logReg_wf,
new_data=test_data,
type="class")
# Export processed dataset
vroom_write(x = amazon_predictions, file = "./amazon_logReg.csv", delim = ",")
amazon_predictions %>%
rename(.pred_class = Action) %>%
bind_cols(id)
amazon_predictions %>%
rename(.pred_class = Action) %>%
bind_cols(test_data$id)
final_predictions <- amazon_predictions %>%
rename(.pred_class = Action) %>%
bind_cols(test_data %>% select(id))
final_predictions <- amazon_predictions %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id))
setwd("~/GitHub/AmazonEmployeeAccess")
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_logReg.csv", delim = ",")
final_predictions <- amazon_predictions %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id)) %>%
mutate(across(everything(), ~replace_na(., 0))) %>%
select(id, Action)
final_predictions <- amazon_predictions %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id)) %>%
mutate(across(Action, ~replace_na(., 0))) %>%
select(id, Action)
final_predictions <- amazon_predictions %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id)) %>%
mutate(Action = as.numeric(as.character(Action))) %>%
mutate(Action = replace_na(Action, 0)) %>%
select(id, Action)
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_logReg.csv", delim = ",")
?select_best()
install.packages("embed")
library(embed)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(
ACTION = as.factor(ACTION),
across(where(is.numeric) & !all_of("ACTION"), as.factor)
)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(ACTION))
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = L) ## L^2 total tuning possibilities
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train_data, v = K, repeats=1)
## Split data for CV
folds <- vfold_cv(train_data, v = 4, repeats=1)
## Run the CV
CV_results <- amazon_workflow %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc, f_meas, sens, recall, spec,
precision, accuracy)) #Or leave metrics NULL
## Run the CV
CV_results <- logReg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc, f_meas, sens, recall, spec,
precision, accuracy)) #Or leave metrics NULL
## Run the CV
CV_results <- logReg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
logReg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=myDataSet)
## Finalize the Workflow & fit it
final_wf <-
logReg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data, type = "class") %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id)) %>%
mutate(
Action = as.numeric(as.character(Action)),
Action = replace_na(Action, 0)
) %>%
select(id, Action)
View(final_predictions)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data, type = "prob") %>%
rename(Action = .pred_class) %>%
bind_cols(test_data %>% select(id)) %>%
mutate(
Action = as.numeric(as.character(Action)),
Action = replace_na(Action, 0)
) %>%
select(id, Action)
View(final_predictions)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(
ACTION = as.factor(ACTION),
across(where(is.numeric) & !all_of("ACTION"), as.factor)
)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(ACTION))
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train_data, v = 4, repeats=1)
## Run the CV
CV_results <- logReg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
logReg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data, type = "prob") %>%
bind_cols(test_data %>% select(id)) %>%
rename(Action = .pred_1) %>%   # Assuming you want P(ACTION = 1)
select(id, Action)
View(final_predictions)
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_logReg.csv", delim = ",")
setwd("~/GitHub/AmazonEmployeeAccess")
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_pen_logReg.csv", delim = ",")
View(bestTune)
View(bestTune)
View(CV_results)
View(folds)
bestTune
CV_results
logRegModel <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train_data, v = 3, repeats=1)
## Run the CV
CV_results <- logReg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
logReg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data, type = "prob") %>%
bind_cols(test_data %>% select(id)) %>%
rename(Action = .pred_1) %>%   # Assuming you want P(ACTION = 1)
select(id, Action)
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_pen_mix_logReg.csv", delim = ",")
bestTune
View(CV_results)
logRegModel <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train_data, v = 3, repeats=1)
## Run the CV
CV_results <- logReg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
logReg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
View(bestTune)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data, type = "prob") %>%
bind_cols(test_data %>% select(id)) %>%
rename(Action = .pred_1) %>%   # Assuming you want P(ACTION = 1)
select(id, Action)
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_pen_mix_logReg.csv", delim = ",")
ssh maaron4@stat-u01.byu.edu
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(
ACTION = as.factor(ACTION),
across(where(is.numeric) & !all_of("ACTION"), as.factor)
)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(ACTION))
logRegModel <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel)
# Export processed dataset
vroom_write(x = final_predictions, file = "./amazon_pen_mix_batch_logReg.csv", delim = ",")
setwd("~/GitHub/AmazonEmployeeAccess")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
