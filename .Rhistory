levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./Forest_LinearPreds3.csv", delim = ",")
library(bonsai)
install.packages("bonsai")
install.packages("lightgbm")
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
install.packages("dbarts")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 2) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(trees(range = c(50, 300)), levels = 4)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./BARTPreds.csv", delim = ",")
library(agua)
library(bonsai)
library(lightgbm)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Initialize an h2o session
h2o::h2o.init()
qt(p = .95, df = 8, lower.tail = TRUE)
qt(p = .05, df = 8, lower.tail = TRUE)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
dplyr::glimpse(train_data)
skimr::skim(train_data)
DataExplorer::plot_intro(train_data)
DataExplorer::plot_correlation(train_data)
DataExplorer::plot_bar(train_data)
DataExplorer::plot_histrograms(train_data)
DataExplorer::plot_correlation(train_data)
DataExplorer::plot_intro(train_data)
DataExplorer::plot_missing(train_data)
GGally::ggpairs(train_data)
MGR_hist <- ggplot(data = train_data, aes(x = MGR_ID)) +
geom_histogram()
MGR_hist
MGR_and_Action <- ggplot(data = train_data, aes(x = MGR_ID, y = ACTION)) +
geom_point() +
geom_smooth()
MGR_and_Action
MGR_Approvals <- ggplot(data = train_data, aes(x = factor(MGR_ID), y = ACTION)) +
geom_jitter(width = 0.2, height = 0.05, alpha = 0.5) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(x = "Manager ID", y = "Approval (0 or 1)",
title = "Individual Approvals by Manager") +
theme_minimal()
MGR_Approvals
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
dplyr::glimpse(train_data)
skimr::skim(train_data)
DataExplorer::plot_correlation(train_data)
DataExplorer::plot_bar(train_data)
DataExplorer::plot_missing(train_data)
GGally::ggpairs(bike_data)
MGR_Approvals <- ggplot(data = train_data, aes(x = factor(MGR_ID), y = ACTION)) +
geom_jitter(width = 0.2, height = 0.05, alpha = 0.5) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(x = "Manager ID", y = "Approval (0 or 1)",
title = "Individual Approvals by Manager") +
theme_minimal()
MGR_Approvals
MGR_hist <- ggplot(data = train_data, aes(x = MGR_ID)) +
geom_histogram()
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: collapse rare factor levels (<0.1% frequency) into "other"
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Step 2: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors())
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
test_processed <- bake(rec_prep, new_data = test_data)
## Write out the file
vroom_write(x=test_processed, file="./baked_data.csv", delim=",")
setwd("~/GitHub/AmazonEmployeeAccess")
## Write out the file
vroom_write(x=test_processed, file="./baked_data.csv", delim=",")
test_processed <- bake(rec_prep, new_data = train_data)
## Write out the file
vroom_write(x=test_processed, file="./format_data.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: collapse rare factor levels (<0.1% frequency) into "other"
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Step 2: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors())
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
processed_data <- bake(rec_prep, new_data = NULL)
## Write out the file
vroom_write(x=processed_data, file="./processed_data.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors())
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors()) %>%
# Step 2: collapse rare factor levels (<0.1% frequency) into "other"
step_other(all_nominal_predictors(), threshold = 0.001, other = "other")
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
processed_data <- bake(rec_prep, new_data = train_data)
## Write out the file
vroom_write(x=processed_data, file="./processed_data.csv", delim=",")
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors()) %>%
# Step 2: collapse rare factor levels (<0.1% frequency) into "other"
step_other(all_nominal_predictors(), threshold = 0.001, other = "other")
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
processed_data <- bake(rec_prep, new_data = train_data)
## Write out the file
vroom_write(x=processed_data, file="./processed_data.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Step 1: collapse rare factor levels (<0.1% frequency) into "other"
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Step 2: create dummy variables (one-hot encoding)
step_dummy(all_nominal_predictors())
# Prepare (learn preprocessing rules)
rec_prep <- my_recipe %>%
prep(training = train_data, retain = TRUE)
# Apply transformations to your training data
processed_data <- bake(rec_prep, new_data = NULL)
# Export to CSV
vroom_write(x = processed_data, file = "./processed_data.csv", delim = ",")
# Make sure categorical variables are factors
train_data <- train_data %>%
mutate(across(where(is.character), as.factor))
# Make sure categorical variables are factors
train_data <- train_data %>%
mutate(across(where(is.nominal), as.factor))
# Define your recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare factor levels (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Create dummy (one-hot) variables
step_dummy(all_nominal_predictors())
# Prep and bake
rec_prep <- my_recipe %>%
prep(training = train_data, retain = TRUE)
processed_data <- bake(rec_prep, new_data = NULL)
# Write out processed dataset
vroom_write(x = processed_data, file = "./processed_data.csv", delim = ",")
# Write out processed dataset
vroom_write(x = processed_data, file = "./processed_data.csv", delim = ",")
train_data <- train_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
# Prep and bake
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
processed_data <- bake(rec_prep, new_data = NULL)
# Export processed dataset
vroom_write(x = processed_data, file = "./processed_train_data.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
test_data <- test_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
# Prep and bake
rec_prep <- my_recipe %>% prep(training = train_data, retain = TRUE)
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(rec_prep) %>%
add_model(logRegModel) %>%
fit(data=train_data)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
train_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\train.csv")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\AmazonEmployeeAccess\\test.csv")
train_data <- train_data %>%
mutate(across(where(is.numeric) & !all_of("ACTION"), as.factor))
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# One-hot encode
step_dummy(all_nominal_predictors())
logRegModel <- logistic_reg() %>%
set_engine("glm")
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
logReg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(logRegModel) %>%
fit(data=train_data)
