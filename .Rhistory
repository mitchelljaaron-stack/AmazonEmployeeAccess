installr(GUI = TRUE)
installr(GUI = TRUE)
# Bike Share Analysis Using Random Forests
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Random Forest
my_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees= 1000) %>% #Type of model
set_engine("ranger") %>% # What R function to use
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 2)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./Forest_LinearPreds.csv", delim = ",")
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./Forest_LinearPreds2.csv", delim = ",")
vroom_write(kaggle_submission, "./Forest_LinearPreds2.csv", delim = ",")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./Forest_LinearPreds2.csv", delim = ",")
# Bike Share Analysis Using Random Forests
library(ranger)
library(tidyverse)
# Bike Share Analysis Using Random Forests
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Random Forest
my_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees= 500) %>% #Type of model
set_engine("ranger") %>% # What R function to use
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./Forest_LinearPreds3.csv", delim = ",")
library(bonsai)
install.packages("bonsai")
install.packages("lightgbm")
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
install.packages("dbarts")
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 2) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(mtry(range = c(1, 15)),
min_n(),
levels = 3)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(trees(range = c(50, 300)), levels = 4)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./BARTPreds.csv", delim = ",")
library(agua)
library(bonsai)
library(lightgbm)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Initialize an h2o session
h2o::h2o.init()
qt(p = .95, df = 8, lower.tail = TRUE)
qt(p = .05, df = 8, lower.tail = TRUE)
MOE <- qt(0.975, df = 26) * (126.3 / (sqrt(27)))
lower bound <- 364.3 - MOE
lower_bound <- 364.3 - MOE
upper_bound <- 364.3 + MOE
qt(0.975, df = 26)
install_tensorflow()
install_tensorflow()
library(tensorflow)
library(discrim)
library(glmnet)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
train_data <- vroom("train.csv")
setwd("~/GitHub/AmazonEmployeeAccess")
library(discrim)
library(glmnet)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
library(tensorflow)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
train_data <- train_data %>%
mutate(
ACTION = as.factor(ACTION),
across(where(is.numeric) & !all_of("ACTION"), as.factor)
)
test_data <- test_data %>%
mutate(across(where(is.numeric), as.factor))
## nn model
nn_model <- mlp(hidden_units = tune(),
epochs = 50) %>%
set_engine("keras") %>%
set_mode("classification")
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_range(all_numeric_predictors(), min=0, max=1)
nn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
## Grid of values to tune over
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 10)),
levels=3)
## Split data for CV
folds <- vfold_cv(train_data, v = 3, repeats=1)
## Run the CV
CV_results <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics = metric_set(roc_auc, accuracy))
py_require("tensorflow")
py_require_legacy_keras()
## Run the CV
CV_results <- nn_wf %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
library(reticulate)
py_install("tensorflow==2.15.0", pip = TRUE)
py_install("keras==2.15.0", pip = TRUE)
library(tensorflow)
library(keras)
tensorflow::tf_config()
library(reticulate)
library(discrim)
library(tensorflow)
library(keras)
tensorflow::tf_config()
# Neural Networks Logistic Predictions
install_tensorflow(version = "2.15.0")
# Neural Networks Logistic Predictions
install_tensorflow(version = "2.15.0")
library(tensorflow)
# Neural Networks Logistic Predictions
install_tensorflow(version = "2.15.0")
library(reticulate)
install_python(version = "3.10.11")
library(reticulate)
install_python(version = "3.10.11")
system("git --version")
system("git --version")
system("git --version")
system("git --version")
system("git --version")
